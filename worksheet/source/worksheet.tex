\documentclass[12pt]{article}
\usepackage{amsfonts, epsfig}
\usepackage[authoryear]{natbib}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{slashbox}
\pagestyle{fancy}
\lfoot{\texttt{comsm0021.github.io}}
\lhead{Neural Information Processing - 1\_information\_theory\_I - Conor}
\rhead{\thepage}
\cfoot{}
\begin{document}

\section*{Worksheet} 

Many of these problems are taken from the excellent text book Cover
and Thomas. Although the questions do vary a bit in difficulty each is
worth two marks. Q3 is probably harder than Q4.

\subsection*{Q1 - marginal and conditional distributions}

Work out the marginal probability distributions and the $x=a$ conditional probability distribution $P(Y|X=a)$ for
\begin{center}
\begin{tabular}{c|cc}
\backslashbox{$Y$}{$X$}&$a$&$b$\\
\hline
1&$\frac{1}{3}$&$\frac{1}{6}$\\
2&0&$\frac{1}{4}$\\
3&$\frac{1}{8}$&$\frac{1}{8}$
\end{tabular}
\end{center}

\subsection*{Q2 - working out entropy}

A fair coin is flipped until the first head occurs. Let $X$ denote the
number of flips required.
\begin{enumerate}
\item Find the entropy $H(X)$ in bits. The following expressions may be useful:\begin{eqnarray}
\sum_{n=0}^{\infty} r^n&=&\frac{1}{1-r}\cr
\sum_{n=0}^{\infty}nr^n&=&\frac{r}{(1-r)^2}
\end{eqnarray}
\item A random variable $X$ is drawn according to this
  distribution. Find a sequence of yes-no questions of the form, \lq
  Is $X$ contained in the set $S$?\rq{}. Compare $H(X)$ to the
  expected number of questions required to determine $X$. For the most
  efficient sequence, that is the sequence the shortest expected
  number of questions, these two numbers will be the same. You will
  find that the most efficient sequence is very straightforward!
\end{enumerate}

\subsection*{Q3 - A puzzle which lends itself to information type reasoning}

Suppose that you have $n$ coins, among which there may or may not be
one counterfeit coin. If there is a counterfeit coin it will weigh
either less or more than the other coins. The coins are weighed using
a balance, any number of coins van be put on each side of the balance,
though obviously you will want the same number on each side.
\begin{enumerate}
\item Find an upper bound on the number of coins $n$ so that $k$
  weighings will find the counterfeit coin, if any, and correctly
  declare it to be heavier or lighter.
\item What is the coin-weighing strategy for $k=3$ weighings and 12
  coins,
\end{enumerate}

\subsection*{Q4 - Working out entropy and information}

Let $p(x,y)$ be given by $p(0,0)=p(0,1)=p(1,1)=1/3$ and
$p(1,0)=0$. Find $H(X)$, $H(Y)$, $H(X|Y)$, $H(Y|X)$, $H(X,Y)$,
$H(Y)-H(Y|X)$ and $I(X;Y)$.

\subsection*{Q5 - A question about information in the brain}

Answer just one of these two questions, each is worth equal marks but the
second is much harder than the first, so you'd be better off doing the
first unless you are particularly interested in this topic. Both papers are available in the paper repository in the github.

\begin{enumerate}
\item The original idea of estimating neural information by binning
  spike trains was spread across several papers, but one of the main
  references is \cite{StrongEtAl1998}. One aspect of this paper we
  didn't discuss is the use of extrapolation to estimate the
  information as the number of samples becomes large based on the
  behaviour for smaller numbers of samples. Can you give a short, up
  to five line, summary of what this involves.

\item In \cite{NemenmanEtAl2004} there is a deep commentary on how
  information in neural data is computed. This is a very difficult
  paper and the mathematics towards the end is hard. The aim of this
  question is to read the paper and offer a three or four line overall
  summary of what the paper is trying to do.
\end{enumerate}

\bibliographystyle{apa}
\bibliography{../../source/bibliography}{}

\end{document}

